{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import seq2seq_LSTM\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the number of loops And Locate the bassline and drum dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbars = 2 \n",
    "\n",
    "bassline_dataset_file = \"../data/bassline_size_50[WithOffet]_translated_to_midi_36-\"+str(int(nbars))+\"bars.txt\"\n",
    "drum_dataset_file = \"../data/drum_size_50.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture (Please understand the code before modifying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Bassline Unique Words: \n",
      "\t\t 36\n",
      "Number of Drum Unique Words: \n",
      "\t\t 256\n",
      "Bassline Unique Words:  \n",
      "\t\t  [   0   21   22   23   24   25   26   27   28   29   30   31   32   33\n",
      "   34   35   36   37   38   39   40   41   42   43   44   45   46   47\n",
      "   48   49   50   51   54 1000]\n",
      "drum_dataset_int_words_tokens \n",
      " {0: 1, 1: 2, 3: 3, 5: 4, 6: 5, 7: 6, 8: 7, 135: 8, 15: 9, 143: 10, 16: 11, 20: 12, 23: 13, 24: 14, 31: 15, 160: 16, 32: 17, 159: 18, 39: 19, 47: 20, 56: 21, 63: 22, 192: 23, 199: 24, 71: 25, 200: 26, 207: 27, 88: 28, 216: 29, 95: 30, 224: 31, 223: 32, 96: 33, 228: 34, 231: 35, 232: 36, 234: 37, 127: 38, 239: 39, 240: 40, 247: 41, 248: 42, 249: 43, 251: 44, 252: 45, 253: 46, 255: 47}\n",
      "target_words_tokens \n",
      " {0: 1, 21: 2, 22: 3, 23: 4, 24: 5, 25: 6, 26: 7, 27: 8, 28: 9, 29: 10, 30: 11, 31: 12, 32: 13, 33: 14, 34: 15, 35: 16, 36: 17, 37: 18, 38: 19, 39: 20, 40: 21, 41: 22, 42: 23, 43: 24, 44: 25, 45: 26, 46: 27, 47: 28, 48: 29, 49: 30, 50: 31, 51: 32, 54: 33, 1000: 34}\n",
      "target_in_words_tokens \n",
      " {0: 1, 21: 2, 22: 3, 23: 4, 24: 5, 25: 6, 26: 7, 27: 8, 28: 9, 29: 10, 30: 11, 31: 12, 32: 13, 33: 14, 34: 15, 35: 16, 36: 17, 37: 18, 38: 19, 39: 20, 40: 21, 41: 22, 42: 23, 43: 24, 44: 25, 45: 26, 46: 27, 47: 28, 48: 29, 49: 30, 50: 31, 51: 32, 'S': 33, 54: 34, 1000: 35}\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, None, 256)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None, 36)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 128), (None, 197120      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 128),  84480       input_6[0][0]                    \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 36)     4644        lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 286,244\n",
      "Trainable params: 286,244\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'create'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-96aa06e82fd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# plot_model(train, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_plot.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'create'"
     ]
    }
   ],
   "source": [
    "drum_dataset_0b, drum_dataset_int, bassline_dataset = seq2seq_LSTM.get_sequences(bassline_dataset_file, drum_dataset_file)\n",
    "\n",
    "output_cardinality = len(np.unique(bassline_dataset))+2 # <<<<<<<<<<<<<<<<<<<< +1 is for the starting character\n",
    "input_cardinality = 2**8 # <<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "print(\"Number of Bassline Unique Words: \\n\\t\\t\", output_cardinality)\n",
    "\n",
    "print(\"Number of Drum Unique Words: \\n\\t\\t\", input_cardinality)\n",
    "print(\"Bassline Unique Words:  \\n\\t\\t \", np.unique(bassline_dataset))\n",
    "\n",
    "# configure problem\n",
    "n_features_drums = input_cardinality           # 2^8 different unique drum combinations #Cardinality of Input\n",
    "n_features_bassline = output_cardinality        #Cardinality of Output\n",
    "n_units = 128\n",
    "\n",
    "# define model\n",
    "train, infenc, infdec = seq2seq_LSTM.define_models(input_cardinality, output_cardinality, n_units)\n",
    "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "encoder_input, decoder_input, decoder_output, drum_dataset_int_words_tokens, target_words_tokens, target_in_words_tokens = seq2seq_LSTM.get_dataset(drum_dataset_int, \n",
    "                                                                                                                                                    bassline_dataset, \n",
    "                                                                                                                                                    padding_value = \"S\", \n",
    "                                                                                                                                                    input_cardinality=input_cardinality, \n",
    "                                                                                                                                                    output_cardinality=output_cardinality)\n",
    "\n",
    "print (\"drum_dataset_int_words_tokens \\n\", drum_dataset_int_words_tokens)\n",
    "print (\"target_words_tokens \\n\", target_words_tokens)\n",
    "print (\"target_in_words_tokens \\n\", target_in_words_tokens)\n",
    "\n",
    "# Show Summary\n",
    "train.summary()\n",
    "\n",
    "# plot_model(train, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(train, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "SVG(model_to_dot(train).create(prog='dot', format='svg'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the number of Epochs used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train.fit([encoder_input, decoder_input], decoder_output, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Encoder/Decoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infenc.save(\"infenc_epoch_\"+str(epochs)+\"_InputCar_\"+str(input_cardinality)+\n",
    "            \"_OutputCar_\"+str(output_cardinality)+\"_n_units_\"+str(n_units)+\".h5\")\n",
    "infdec.save(\"infdec_epoch_\"+str(epochs)+\"_InputCar_\"+str(input_cardinality)+\n",
    "            \"_OutputCar_\"+str(output_cardinality)+\"_n_units_\"+str(n_units)+\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['acc'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('mvae': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6ff0637e2c28195eff2d12b0ff4978a7b09a275dd00ac19b2e3ad0b3d9910e73"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}